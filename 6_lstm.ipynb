{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 99999000 | Sample: ons anarchists advocate social relations based upon voluntary as\n",
      "Validation set size: 1000 | Sample:  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(\"Train set size: \"+str(train_size)+\" | Sample: \"+train_text[:64])\n",
    "print(\"Validation set size: \"+str(valid_size)+\" | Sample: \"+valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' ' #27\n",
    "first_letter = ord(string.ascii_lowercase[0]) #97\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    #print(segment)\n",
    "    #print(self._cursor )\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      #print(\"batch[\"+str(b)+\", char2id(self._text[self._cursor[\"+str(b)+\"]])] = 1.0\")\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    #print(batch)\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    #print(batches)\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  #for x in zip(s, characters(batches[0])):\n",
    "  #      print(x)\n",
    "  #print(\"\")\n",
    "  #for x in zip(s, characters(batches[1])):\n",
    "  #      print(x)\n",
    "        \n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    #print(s)\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1) # Random value\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  \n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print('Initialized')\n",
    "      print(\"\")\n",
    "      mean_loss = 0\n",
    "      for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "          feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "          if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "          # The mean loss is an estimate of the loss over the last few batches.\n",
    "          print(\"Step \"+str(step)+\":\")\n",
    "          print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "          mean_loss = 0\n",
    "          labels = np.concatenate(list(batches)[1:])\n",
    "          print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "          if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "              feed = sample(random_distribution())\n",
    "              sentence = characters(feed)[0]\n",
    "              reset_sample_state.run()\n",
    "              for _ in range(79):\n",
    "                prediction = sample_prediction.eval({sample_input: feed})\n",
    "                feed = sample(prediction)\n",
    "                sentence += characters(feed)[0]\n",
    "              print(sentence)\n",
    "            print('=' * 80)\n",
    "          # Measure validation set perplexity.\n",
    "          reset_sample_state.run()\n",
    "          valid_logprob = 0\n",
    "          for _ in range(valid_size):\n",
    "            b = valid_batches.next()\n",
    "            predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "            valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "          print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "          print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Step 0:\n",
      "Average loss at step 0: 3.296184 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      " fspormfa jkccxkwgxh ilefun fneelzj qee lcagkc mxpi lzrsdhvmmuv  ofl   cbpaiwcok\n",
      "isoyniq  nnp xqgpa njflgqqjchnqdkdofnitgig xasmeaczmd xahnymroi jzgvodklrntnbegv\n",
      "covnpiopmlup ntu emnua hamqjuuzcw tghieiigen ng hdser ahnc alniskiswgcvoszsaaxjw\n",
      "lmxur q xhhtlou fpejn tlr fupsffutltom ecdmpqibcqjlesoamul puufzijoersishrelodl \n",
      "tmmc ttosiqdajyy ngjavpahnbaw db ummlstzaujytnsrfmtdeisothpc vn nurpasygvdtpfqqd\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "\n",
      "Step 100:\n",
      "Average loss at step 100: 2.597760 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.99\n",
      "Validation set perplexity: 10.29\n",
      "\n",
      "Step 200:\n",
      "Average loss at step 200: 2.261030 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.66\n",
      "Validation set perplexity: 8.59\n",
      "\n",
      "Step 300:\n",
      "Average loss at step 300: 2.097619 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.33\n",
      "Validation set perplexity: 8.03\n",
      "\n",
      "Step 400:\n",
      "Average loss at step 400: 1.991213 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.70\n",
      "\n",
      "Step 500:\n",
      "Average loss at step 500: 1.927181 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 6.97\n",
      "\n",
      "Step 600:\n",
      "Average loss at step 600: 1.900616 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.90\n",
      "\n",
      "Step 700:\n",
      "Average loss at step 700: 1.855191 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.56\n",
      "\n",
      "Step 800:\n",
      "Average loss at step 800: 1.816292 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.30\n",
      "\n",
      "Step 900:\n",
      "Average loss at step 900: 1.825479 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 6.18\n",
      "\n",
      "Step 1000:\n",
      "Average loss at step 1000: 1.821341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "mentaid acrav a lare islecs upobs auroused our libotion a the nuducted the refer\n",
      "urces in sheope and evelefse in iny southemary relope richies tolber dis do daot\n",
      "wed in garings refines priinecs mests a a his bremerage in chille furnmenth fela\n",
      "cled parlicaple coundeding wore a rasticgust in pranudies of of etrate wort as d\n",
      "ceesed he sie of the  or clitors fillo copredics hratery was duss actoric widite\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "\n",
      "Step 1100:\n",
      "Average loss at step 1100: 1.776195 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.81\n",
      "\n",
      "Step 1200:\n",
      "Average loss at step 1200: 1.755074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.59\n",
      "\n",
      "Step 1300:\n",
      "Average loss at step 1300: 1.735526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.55\n",
      "\n",
      "Step 1400:\n",
      "Average loss at step 1400: 1.745015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.52\n",
      "\n",
      "Step 1500:\n",
      "Average loss at step 1500: 1.738483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.45\n",
      "\n",
      "Step 1600:\n",
      "Average loss at step 1600: 1.747875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.44\n",
      "\n",
      "Step 1700:\n",
      "Average loss at step 1700: 1.713164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.29\n",
      "\n",
      "Step 1800:\n",
      "Average loss at step 1800: 1.679318 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.14\n",
      "\n",
      "Step 1900:\n",
      "Average loss at step 1900: 1.652534 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.17\n",
      "\n",
      "Step 2000:\n",
      "Average loss at step 2000: 1.700880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "tent of di war six seven inclusider producturadex created lowest bork and toad t\n",
      "f in of d for and complents three zero nine four five bistifion reling abcaivion\n",
      "xic that is common propladity with by to them emple mattof on timen one one eigh\n",
      "n man introdical bagitle the native enging on the matter encodors is many bajagn\n",
      "oung moloes rick hincul yeasy and ncamacte s recold peb lingly in de of thinigho\n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "\n",
      "Step 2100:\n",
      "Average loss at step 2100: 1.690792 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.95\n",
      "\n",
      "Step 2200:\n",
      "Average loss at step 2200: 1.685946 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 4.97\n",
      "\n",
      "Step 2300:\n",
      "Average loss at step 2300: 1.642014 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.88\n",
      "\n",
      "Step 2400:\n",
      "Average loss at step 2400: 1.661188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.76\n",
      "\n",
      "Step 2500:\n",
      "Average loss at step 2500: 1.675735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Step 2600:\n",
      "Average loss at step 2600: 1.651907 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.54\n",
      "\n",
      "Step 2700:\n",
      "Average loss at step 2700: 1.659450 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 2800:\n",
      "Average loss at step 2800: 1.652475 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Step 2900:\n",
      "Average loss at step 2900: 1.650804 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 4.72\n",
      "\n",
      "Step 3000:\n",
      "Average loss at step 3000: 1.653833 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "for six is was proptinity esuth herojams to contingstrughtra bols included mannt\n",
      "trad johns one nine seven nine  willwann an expeoor rectcome and played heralost\n",
      "tiogly in a meas and yotional such diritated brotraty as part could not as calle\n",
      "x copst currence of normbes chus gromourishant of absper juaters and shors is ar\n",
      "wiff kaden memborienn to informaning sovietes callettly curpurnien one newshor w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Step 3100:\n",
      "Average loss at step 3100: 1.628480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 3200:\n",
      "Average loss at step 3200: 1.650193 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.52\n",
      "\n",
      "Step 3300:\n",
      "Average loss at step 3300: 1.639072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.54\n",
      "\n",
      "Step 3400:\n",
      "Average loss at step 3400: 1.667942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 3500:\n",
      "Average loss at step 3500: 1.659024 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 3600:\n",
      "Average loss at step 3600: 1.669186 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.43\n",
      "\n",
      "Step 3700:\n",
      "Average loss at step 3700: 1.643918 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.47\n",
      "\n",
      "Step 3800:\n",
      "Average loss at step 3800: 1.646611 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 3900:\n",
      "Average loss at step 3900: 1.638667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.48\n",
      "\n",
      "Step 4000:\n",
      "Average loss at step 4000: 1.652447 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "chine e may brign duel aba his viember one five cervitu interactive his become a\n",
      "lees six the genout scr the us lare fiss rad maurnai since creess of besests bra\n",
      "jemi production a cutuomal braed from she peter rbitacre modername as prespeed i\n",
      "der semetht one zero seven three jess it mae a bys hunnened on the souet proypra\n",
      "mia vech usent of usue in the sshimet so vasici genes have schn slass depension \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.45\n",
      "\n",
      "Step 4100:\n",
      "Average loss at step 4100: 1.634634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Step 4200:\n",
      "Average loss at step 4200: 1.638370 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Step 4300:\n",
      "Average loss at step 4300: 1.617599 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.34\n",
      "\n",
      "Step 4400:\n",
      "Average loss at step 4400: 1.610473 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.23\n",
      "\n",
      "Step 4500:\n",
      "Average loss at step 4500: 1.617926 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Step 4600:\n",
      "Average loss at step 4600: 1.614775 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.54\n",
      "\n",
      "Step 4700:\n",
      "Average loss at step 4700: 1.627939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.40\n",
      "\n",
      "Step 4800:\n",
      "Average loss at step 4800: 1.633414 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.41\n",
      "\n",
      "Step 4900:\n",
      "Average loss at step 4900: 1.634146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Step 5000:\n",
      "Average loss at step 5000: 1.609813 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "================================================================================\n",
      " or the japa it low as an d bants of him ivaslaruesto two s octerulg the navalit\n",
      "king zerenath at all two dess humujath right courned has plangs as between confi\n",
      "pend s genity of genioe even six that elections obself extanjes countre of the j\n",
      "bus imphowive peoplay a in equarnerly yeast hapes the was they two s seaster may\n",
      "kn typest kenolitient itso p one zero undibzy fiss to three phed crilies otherse\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "\n",
      "Step 5100:\n",
      "Average loss at step 5100: 1.608597 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.36\n",
      "\n",
      "Step 5200:\n",
      "Average loss at step 5200: 1.592964 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.32\n",
      "\n",
      "Step 5300:\n",
      "Average loss at step 5300: 1.583721 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.31\n",
      "\n",
      "Step 5400:\n",
      "Average loss at step 5400: 1.579600 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.28\n",
      "\n",
      "Step 5500:\n",
      "Average loss at step 5500: 1.568130 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.26\n",
      "\n",
      "Step 5600:\n",
      "Average loss at step 5600: 1.583162 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Step 5700:\n",
      "Average loss at step 5700: 1.571232 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.23\n",
      "\n",
      "Step 5800:\n",
      "Average loss at step 5800: 1.585098 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Step 5900:\n",
      "Average loss at step 5900: 1.577062 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.22\n",
      "\n",
      "Step 6000:\n",
      "Average loss at step 6000: 1.546621 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "dict her iv to triety and souther buth and sidnet part also manxens one seven tw\n",
      "p kour agus of mortaht a mendement while a u one time to often bases rare alana \n",
      "nellys in compleme subrectin to the long bepricished court permination capitor a\n",
      "ham refuges of nine nine six qua norg one zero zero zero refine zero threapes he\n",
      "jopolanda mod voes is it death howng juny ataso implation is frames cri band in \n",
      "================================================================================\n",
      "Validation set perplexity: 4.20\n",
      "\n",
      "Step 6100:\n",
      "Average loss at step 6100: 1.566744 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.21\n",
      "\n",
      "Step 6200:\n",
      "Average loss at step 6200: 1.534706 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.19\n",
      "\n",
      "Step 6300:\n",
      "Average loss at step 6300: 1.543331 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Step 6400:\n",
      "Average loss at step 6400: 1.541991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Step 6500:\n",
      "Average loss at step 6500: 1.561462 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Step 6600:\n",
      "Average loss at step 6600: 1.593628 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Step 6700:\n",
      "Average loss at step 6700: 1.578951 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.18\n",
      "\n",
      "Step 6800:\n",
      "Average loss at step 6800: 1.605718 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.17\n",
      "\n",
      "Step 6900:\n",
      "Average loss at step 6900: 1.582960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.23\n",
      "\n",
      "Step 7000:\n",
      "Average loss at step 7000: 1.576689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "mariele bassonm of internation and leaging have singooks over the hay the monote\n",
      "entine former issellow is witholf control chivements pole the into the tim cluec\n",
      "ratic black betal c one nine four six seven alone releatymono to be a which in g\n",
      "veters it and received colum compamits one nic for returtries were was there arg\n",
      "let terrise viemon the art are lishd coppere vatarist and studgor feramy be orig\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "\n",
      "CPU times: user 2min 10s, sys: 29.5 s, total: 2min 40s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Big Matrix:\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    \n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_m = all_gates[:, 0:num_nodes]\n",
    "    forget_m = all_gates[:, num_nodes:2*num_nodes]\n",
    "    update_m = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    output_m = all_gates[:, 3*num_nodes:]\n",
    "\n",
    "    input_gate2 = tf.sigmoid(input_m)\n",
    "    forget_gate2 = tf.sigmoid(forget_m)\n",
    "    state2 = forget_gate2 * state + input_gate2 * tf.tanh(update_m)\n",
    "    output_gate2 = tf.sigmoid(output_m)\n",
    "    \n",
    "    return output_gate2 * tf.tanh(state2), state2\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "  \n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph(graph):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      tf.global_variables_initializer().run()\n",
    "      print('Initialized')\n",
    "      print(\"\")\n",
    "      mean_loss = 0\n",
    "      for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "          feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "          if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "          # The mean loss is an estimate of the loss over the last few batches.\n",
    "          print(\"Step \"+str(step)+\":\")\n",
    "          print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "          mean_loss = 0\n",
    "          labels = np.concatenate(list(batches)[1:])\n",
    "          print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "          if step % (summary_frequency * 10) == 0:\n",
    "            # Generate some samples.\n",
    "            print('=' * 80)\n",
    "            for _ in range(5):\n",
    "              feed = sample(random_distribution())\n",
    "              sentence = characters(feed)[0]\n",
    "              reset_sample_state.run()\n",
    "              for _ in range(79):\n",
    "                prediction = sample_prediction.eval({sample_input: feed})\n",
    "                feed = sample(prediction)\n",
    "                sentence += characters(feed)[0]\n",
    "              print(sentence)\n",
    "            print('=' * 80)\n",
    "          # Measure validation set perplexity.\n",
    "          reset_sample_state.run()\n",
    "          valid_logprob = 0\n",
    "          for _ in range(valid_size):\n",
    "            b = valid_batches.next()\n",
    "            predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "            valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "          print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "          print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Step 0:\n",
      "Average loss at step 0: 3.292948 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.92\n",
      "================================================================================\n",
      "so wlibttkp fe  fxaxxhknhat x heenilbtkuu sadskigulx dety if qj awtjoestp cqnzht\n",
      "ctkiirmsreycosqylge   hmlo  pce eiumavtentadogwivnd  t sxgt nqcdirpzxa ndt  twrm\n",
      "er txjzvvuqoo vvwtctm iort mhd t na timdepdu eniw bzsneue rdhjvos  jq   ae ajvxo\n",
      "oillyyh  qvixogyisuyjok kay llqf ey rat twnve eezona kbkrnxcnltrzwh iflrdkeoatt \n",
      "neo ovv vntdgeo  priafu ruyeere nxta nahmttvjxmt aw esr hdtwbt hoohhv gecvjiw fn\n",
      "================================================================================\n",
      "Validation set perplexity: 20.15\n",
      "\n",
      "Step 100:\n",
      "Average loss at step 100: 2.595035 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.55\n",
      "Validation set perplexity: 10.46\n",
      "\n",
      "Step 200:\n",
      "Average loss at step 200: 2.244040 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 8.88\n",
      "\n",
      "Step 300:\n",
      "Average loss at step 300: 2.080216 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 8.01\n",
      "\n",
      "Step 400:\n",
      "Average loss at step 400: 2.023971 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 7.91\n",
      "\n",
      "Step 500:\n",
      "Average loss at step 500: 1.970472 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.23\n",
      "\n",
      "Step 600:\n",
      "Average loss at step 600: 1.893165 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.06\n",
      "\n",
      "Step 700:\n",
      "Average loss at step 700: 1.868405 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.86\n",
      "\n",
      "Step 800:\n",
      "Average loss at step 800: 1.861356 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.66\n",
      "\n",
      "Step 900:\n",
      "Average loss at step 900: 1.838698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 6.49\n",
      "\n",
      "Step 1000:\n",
      "Average loss at step 1000: 1.842927 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "================================================================================\n",
      "quurovic of move distifution also the un which gith adabien b lestal evenzenting\n",
      "d spetenvagur they f e with the moy one nine zero zero five fire founder of the \n",
      "ast with tensmated anvidy of jage was trooged sites by josted h and has wans pro\n",
      "ucic inetreanci and for of the covies caurooks the fican the ecages in the y res\n",
      "igies in ran softrating haw d admonabive ning recation the haverer by grimingeri\n",
      "================================================================================\n",
      "Validation set perplexity: 6.12\n",
      "\n",
      "Step 1100:\n",
      "Average loss at step 1100: 1.800642 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.17\n",
      "\n",
      "Step 1200:\n",
      "Average loss at step 1200: 1.768143 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.12\n",
      "\n",
      "Step 1300:\n",
      "Average loss at step 1300: 1.757240 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.04\n",
      "\n",
      "Step 1400:\n",
      "Average loss at step 1400: 1.761436 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.87\n",
      "\n",
      "Step 1500:\n",
      "Average loss at step 1500: 1.746089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.69\n",
      "\n",
      "Step 1600:\n",
      "Average loss at step 1600: 1.728069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.72\n",
      "\n",
      "Step 1700:\n",
      "Average loss at step 1700: 1.714421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.53\n",
      "\n",
      "Step 1800:\n",
      "Average loss at step 1800: 1.688800 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.38\n",
      "\n",
      "Step 1900:\n",
      "Average loss at step 1900: 1.692428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.40\n",
      "\n",
      "Step 2000:\n",
      "Average loss at step 2000: 1.678745 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "velt for monsdand new hindrank lites own conniened sued bit goveman in eargh eig\n",
      "y sergerman with to al alsover was nems the c eurtus tank kary temperatt post th\n",
      "cay prestation and for three seven externn martes the no vary melimanaured preeu\n",
      "ques for esticule procimates and bul hinderge eight of covent an acvelture platt\n",
      "ger and tom the numpy will of aindanted superssausific lired with was nak u aut \n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "\n",
      "Step 2100:\n",
      "Average loss at step 2100: 1.687940 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.27\n",
      "\n",
      "Step 2200:\n",
      "Average loss at step 2200: 1.708693 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.29\n",
      "\n",
      "Step 2300:\n",
      "Average loss at step 2300: 1.708126 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.33\n",
      "\n",
      "Step 2400:\n",
      "Average loss at step 2400: 1.682462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.39\n",
      "\n",
      "Step 2500:\n",
      "Average loss at step 2500: 1.684567 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.33\n",
      "\n",
      "Step 2600:\n",
      "Average loss at step 2600: 1.671885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.15\n",
      "\n",
      "Step 2700:\n",
      "Average loss at step 2700: 1.686230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.14\n",
      "\n",
      "Step 2800:\n",
      "Average loss at step 2800: 1.678396 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.30\n",
      "\n",
      "Step 2900:\n",
      "Average loss at step 2900: 1.676865 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.17\n",
      "\n",
      "Step 3000:\n",
      "Average loss at step 3000: 1.685042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "ked oil and a j tabio knat even one mi related teltimm ralay nums of some as a h\n",
      "bication of stapo than on man varinely purticlan and to akamin like of tatemoria\n",
      "us useen extloue chioference tongety the clay neda disonan internet f man wiltst\n",
      "x be form is auth veller of syllen lpaus d andhoturait liokors sarp a subinvive \n",
      "s adoctity as legens to to song is ingrigh the evisal encarif the reallved as us\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "\n",
      "Step 3100:\n",
      "Average loss at step 3100: 1.652786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.12\n",
      "\n",
      "Step 3200:\n",
      "Average loss at step 3200: 1.638574 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.87\n",
      "\n",
      "Step 3300:\n",
      "Average loss at step 3300: 1.646210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.97\n",
      "\n",
      "Step 3400:\n",
      "Average loss at step 3400: 1.634185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.90\n",
      "\n",
      "Step 3500:\n",
      "Average loss at step 3500: 1.671770 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.96\n",
      "\n",
      "Step 3600:\n",
      "Average loss at step 3600: 1.651724 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.82\n",
      "\n",
      "Step 3700:\n",
      "Average loss at step 3700: 1.653548 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.99\n",
      "\n",
      "Step 3800:\n",
      "Average loss at step 3800: 1.657175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 4.87\n",
      "\n",
      "Step 3900:\n",
      "Average loss at step 3900: 1.648429 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.81\n",
      "\n",
      "Step 4000:\n",
      "Average loss at step 4000: 1.640646 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "sur edear any the so domnt is  lands born mathobers in one eight zero three four\n",
      "jeen perist accerse of the ferce sobeled scients afferengle in teapte the from t\n",
      " regardhmon overseli delation frence indo of rebul great of chesdistries mascras\n",
      "y genery rights has epingal probleader and ewecksshara longs hugh are enastle eu\n",
      "has revelovicamuarion of the statute there new throuxhutmicial cathome trise b s\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.87\n",
      "\n",
      "Step 4100:\n",
      "Average loss at step 4100: 1.621154 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.74\n",
      "\n",
      "Step 4200:\n",
      "Average loss at step 4200: 1.616255 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.85\n",
      "\n",
      "Step 4300:\n",
      "Average loss at step 4300: 1.620703 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.77\n",
      "\n",
      "Step 4400:\n",
      "Average loss at step 4400: 1.607945 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.83\n",
      "\n",
      "Step 4500:\n",
      "Average loss at step 4500: 1.640885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.95\n",
      "\n",
      "Step 4600:\n",
      "Average loss at step 4600: 1.623149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.01\n",
      "\n",
      "Step 4700:\n",
      "Average loss at step 4700: 1.621409 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.94\n",
      "\n",
      "Step 4800:\n",
      "Average loss at step 4800: 1.607794 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.92\n",
      "\n",
      "Step 4900:\n",
      "Average loss at step 4900: 1.615397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.78\n",
      "\n",
      "Step 5000:\n",
      "Average loss at step 5000: 1.613973 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "h the crother accrestion control allow zero modationes the english leved toging \n",
      "quesdons francish remate apostone in the shope of gorderic titles read soldia wo\n",
      "form of the charies fromicion in the work in the loi hinad in in thin in one nin\n",
      "le foellogy s knownames in compoaling are gore tokefority over city wehes whay h\n",
      "vision largic by nore of milest daitymal poped labrages with the joons modess sp\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "\n",
      "Step 5100:\n",
      "Average loss at step 5100: 1.591884 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.71\n",
      "\n",
      "Step 5200:\n",
      "Average loss at step 5200: 1.592112 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.68\n",
      "\n",
      "Step 5300:\n",
      "Average loss at step 5300: 1.592651 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.63\n",
      "\n",
      "Step 5400:\n",
      "Average loss at step 5400: 1.593320 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Step 5500:\n",
      "Average loss at step 5500: 1.590617 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.59\n",
      "\n",
      "Step 5600:\n",
      "Average loss at step 5600: 1.562103 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.56\n",
      "\n",
      "Step 5700:\n",
      "Average loss at step 5700: 1.581553 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.51\n",
      "\n",
      "Step 5800:\n",
      "Average loss at step 5800: 1.599746 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.53\n",
      "\n",
      "Step 5900:\n",
      "Average loss at step 5900: 1.581182 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.54\n",
      "\n",
      "Step 6000:\n",
      "Average loss at step 6000: 1.582287 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "velanty irreding mushives the discressed by engines weinon an century rescirfari\n",
      "n one throur take wide extimles that heaton an east couded fix the in where taca\n",
      "ar a viologic named and glosathysoks on tree abreticale developoricles hiaging o\n",
      "zer all term trotation and well recording eight one six eight possible the is ke\n",
      "rest of poened instanted benament same to element to nim hart devellete attacker\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "\n",
      "Step 6100:\n",
      "Average loss at step 6100: 1.574148 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.55\n",
      "\n",
      "Step 6200:\n",
      "Average loss at step 6200: 1.586701 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 6300:\n",
      "Average loss at step 6300: 1.584168 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Step 6400:\n",
      "Average loss at step 6400: 1.572167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.60\n",
      "\n",
      "Step 6500:\n",
      "Average loss at step 6500: 1.558333 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.62\n",
      "\n",
      "Step 6600:\n",
      "Average loss at step 6600: 1.598320 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 6700:\n",
      "Average loss at step 6700: 1.567812 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.57\n",
      "\n",
      "Step 6800:\n",
      "Average loss at step 6800: 1.576301 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.61\n",
      "\n",
      "Step 6900:\n",
      "Average loss at step 6900: 1.568840 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "Step 7000:\n",
      "Average loss at step 7000: 1.585267 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "aline of the sypo four six one nine nine versity and the l climated was a all in\n",
      "y up dependuring has died these jasmany lindigy used by capuligard appleter prea\n",
      "de ho tennessifring his kings to insumped that other own bold the wase agailder \n",
      "ars inplased group marysistolis three two seven one people meture praing abrecal\n",
      "ailed yen at as useas that hohetation known jil the censong technimment scoplawe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "\n",
      "CPU times: user 2min 1s, sys: 19.7 s, total: 2min 20s\n",
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices:\n",
    "\n",
    "---\n",
    "\n",
    "Reuse the Seq2SeqModel class in the RNN translate example\n",
    "\n",
    "The solution is based on the posts from \"dtrebbien\" in the udacity forum, thanks to him for his great explanations!\n",
    "\n",
    "Url: https://discussions.udacity.com/t/assignment-6-problem-3-benchmarks/158517\n",
    "\n",
    "And on \"vrasneur\" solution:\n",
    "\n",
    "https://github.com/vrasneur/udacity-deep_learning/blob/master/6_lstm.ipynb\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the char to id conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' ' #27\n",
    "first_letter = ord(string.ascii_lowercase[0]) #97\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample, we will try to reverse all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "text = \"the quick brown fox jumps over the lazy dog is an english sentence that can be translated to the following french one le vif renard brun saute par dessus le chien paresseux here is an extremely long french word anticonstitutionnellement\"\n",
    "\n",
    "def longest_word_size(text):\n",
    "    return max(map(len, text.split()))\n",
    "\n",
    "word_size = longest_word_size(text)\n",
    "print(word_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket\n",
    "                                   size=num_nodes,\n",
    "                                   num_layers=3,\n",
    "                                   max_gradient_norm=5.0,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=0.5,\n",
    "                                   learning_rate_decay_factor=0.99,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in range(batch_size)] #min, max and length\n",
    "    decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in range(batch_size)] #Zero at beginning and end\n",
    "    weights = [np.ones(word_size + 2, dtype=np.float32) for _ in range(batch_size)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        r = random.randint(1, word_size)\n",
    "        # leave at least a 0 at the end\n",
    "        encoder_inputs[i][r:] = 0 # Create random size vectors filled with zeros\n",
    "        # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1] #Reverse the randomized part\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    # 0 is the code for space in char2id()\n",
    "    return word.strip(' ')\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "    correct = 0\n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32)\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32)\n",
    "    target_weights[0,:] = 1.0\n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_)\n",
    "    for i in range(word_size + 1):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "        p = np.argmax(output_logits[i], axis=1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0) #Check if all the logit max are at the beginning\n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p #If finished is True the output is 0 on contrary is p\n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0 #If finished is True the output is 0 on contrary is 1.0\n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '->', output_word, '({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_batch(words):\n",
    "    encoder_inputs = [np.zeros(word_size + 1, dtype=np.int32) for _ in range(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        for j, c in enumerate(word):\n",
    "            encoder_inputs[i][j] = char2id(c)\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    nb_words = len(words)\n",
    "    correct = 0\n",
    "    for i in range(nb_words // batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, nb_words, (float(correct) / nb_words) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_text(nb_steps):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model()\n",
    "        tf.global_variables_initializer().run()\n",
    "        for step in range(nb_steps):\n",
    "            enc_inputs, dec_inputs, weights = get_batch()\n",
    "            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, bucket_id=0, forward_only=False)\n",
    "            if step % 100 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "            if step % 1000 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(text, model, session)\n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(text, model, session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 1 loss: 3.29997\n",
      "* step: 1 loss: 3.29997\n",
      "the (reversed: eht) ->  (KO)\n",
      "quick (reversed: kciuq) ->  (KO)\n",
      "brown (reversed: nworb) ->  (KO)\n",
      "fox (reversed: xof) ->  (KO)\n",
      "jumps (reversed: spmuj) ->  (KO)\n",
      "over (reversed: revo) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "lazy (reversed: yzal) ->  (KO)\n",
      "dog (reversed: god) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "an (reversed: na) ->  (KO)\n",
      "english (reversed: hsilgne) ->  (KO)\n",
      "sentence (reversed: ecnetnes) ->  (KO)\n",
      "that (reversed: taht) ->  (KO)\n",
      "can (reversed: nac) ->  (KO)\n",
      "be (reversed: eb) ->  (KO)\n",
      "translated (reversed: detalsnart) ->  (KO)\n",
      "to (reversed: ot) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "following (reversed: gniwollof) ->  (KO)\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "one (reversed: eno) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "vif (reversed: fiv) ->  (KO)\n",
      "renard (reversed: draner) ->  (KO)\n",
      "brun (reversed: nurb) ->  (KO)\n",
      "saute (reversed: etuas) ->  (KO)\n",
      "par (reversed: rap) ->  (KO)\n",
      "dessus (reversed: sussed) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "chien (reversed: neihc) ->  (KO)\n",
      "paresseux (reversed: xuesserap) ->  (KO)\n",
      "here (reversed: ereh) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "an (reversed: na) ->  (KO)\n",
      "extremely (reversed: ylemertxe) ->  (KO)\n",
      "long (reversed: gnol) ->  (KO)\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "word (reversed: drow) ->  (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> j (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 101 loss: 3.12092\n",
      "* step: 201 loss: 3.08486\n",
      "* step: 301 loss: 2.9225\n",
      "* step: 401 loss: 2.64531\n",
      "* step: 501 loss: 2.42599\n",
      "* step: 601 loss: 2.53625\n",
      "* step: 701 loss: 1.9267\n",
      "* step: 801 loss: 2.04631\n",
      "* step: 901 loss: 2.08517\n",
      "* step: 1001 loss: 1.85052\n",
      "* step: 1001 loss: 1.85052\n",
      "the (reversed: eht) -> hehh (KO)\n",
      "quick (reversed: kciuq) -> kkkuuu (KO)\n",
      "brown (reversed: nworb) -> nnnnbbbb (KO)\n",
      "fox (reversed: xof) -> xxx (KO)\n",
      "jumps (reversed: spmuj) -> jssjjjj (KO)\n",
      "over (reversed: revo) -> vrrvo (KO)\n",
      "the (reversed: eht) -> hehh (KO)\n",
      "lazy (reversed: yzal) -> yyzzzz (KO)\n",
      "dog (reversed: god) -> gggg (KO)\n",
      "is (reversed: si) -> is (KO)\n",
      "an (reversed: na) -> anna (KO)\n",
      "english (reversed: hsilgne) -> hhsssllee (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeeeee (KO)\n",
      "that (reversed: taht) -> tttttt (KO)\n",
      "can (reversed: nac) -> anna (KO)\n",
      "be (reversed: eb) -> eee (KO)\n",
      "translated (reversed: detalsnart) -> ddennnnnnnn (KO)\n",
      "to (reversed: ot) -> too (KO)\n",
      "the (reversed: eht) -> hehh (KO)\n",
      "following (reversed: gniwollof) -> ggooooollll (KO)\n",
      "french (reversed: hcnerf) -> chnnnnrr (KO)\n",
      "one (reversed: eno) -> neeoo (KO)\n",
      "le (reversed: el) -> lell (KO)\n",
      "vif (reversed: fiv) -> ffv (KO)\n",
      "renard (reversed: draner) -> rrrnnnrr (KO)\n",
      "brun (reversed: nurb) -> nnnnbb (KO)\n",
      "saute (reversed: etuas) -> uuuuuu (KO)\n",
      "par (reversed: rap) -> rrppp (KO)\n",
      "dessus (reversed: sussed) -> ssssssss (KO)\n",
      "le (reversed: el) -> lell (KO)\n",
      "chien (reversed: neihc) -> nneeccc (KO)\n",
      "paresseux (reversed: xuesserap) -> xxeeeeeppp (KO)\n",
      "here (reversed: ereh) -> eeeeee (KO)\n",
      "is (reversed: si) -> is (KO)\n",
      "an (reversed: na) -> anna (KO)\n",
      "extremely (reversed: ylemertxe) -> yeeeeeeeeee (KO)\n",
      "long (reversed: gnol) -> gggolll (KO)\n",
      "french (reversed: hcnerf) -> chnnnnrr (KO)\n",
      "word (reversed: drow) -> dddww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> uuunnnnnnneenuuuuuuuuonnnn (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 1101 loss: 2.06547\n",
      "* step: 1201 loss: 1.84482\n",
      "* step: 1301 loss: 1.41762\n",
      "* step: 1401 loss: 1.2686\n",
      "* step: 1501 loss: 1.31197\n",
      "* step: 1601 loss: 0.885547\n",
      "* step: 1701 loss: 0.910632\n",
      "* step: 1801 loss: 1.26628\n",
      "* step: 1901 loss: 1.23909\n",
      "* step: 2001 loss: 1.73729\n",
      "* step: 2001 loss: 1.73729\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kccuq (KO)\n",
      "brown (reversed: nworb) -> nnobb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> sppuj (KO)\n",
      "over (reversed: revo) -> rreo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzzl (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhsilee (KO)\n",
      "sentence (reversed: ecnetnes) -> eecnnenes (KO)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> e (KO)\n",
      "translated (reversed: detalsnart) -> deetallnnrr (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> ggiwwoollf (KO)\n",
      "french (reversed: hcnerf) -> hhnnef (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> drrner (KO)\n",
      "brun (reversed: nurb) -> nrrb (KO)\n",
      "saute (reversed: etuas) -> etuus (KO)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssed (KO)\n",
      "le (reversed: el) -> ee (KO)\n",
      "chien (reversed: neihc) -> nneic (KO)\n",
      "paresseux (reversed: xuesserap) -> xuueesserpp (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yleeeeertee (KO)\n",
      "long (reversed: gnol) -> ggnl (KO)\n",
      "french (reversed: hcnerf) -> hhnnef (KO)\n",
      "word (reversed: drow) -> ddrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnennnottttitnnocin (KO)\n",
      "* correct: 14/40 -> 35.0%\n",
      "\n",
      "* step: 2101 loss: 1.43057\n",
      "* step: 2201 loss: 0.936709\n",
      "* step: 2301 loss: 0.40123\n",
      "* step: 2401 loss: 0.716609\n",
      "* step: 2501 loss: 0.749202\n",
      "* step: 2601 loss: 0.324197\n",
      "* step: 2701 loss: 0.304984\n",
      "* step: 2801 loss: 1.05594\n",
      "* step: 2901 loss: 2.0354\n",
      "* step: 3001 loss: 0.450593\n",
      "* step: 3001 loss: 0.450593\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ntnnnnnnnnottttitsnocitn (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 3101 loss: 0.382187\n",
      "* step: 3201 loss: 0.0880661\n",
      "* step: 3301 loss: 0.246879\n",
      "* step: 3401 loss: 0.843665\n",
      "* step: 3501 loss: 0.767992\n",
      "* step: 3601 loss: 0.338457\n",
      "* step: 3701 loss: 0.307406\n",
      "* step: 3801 loss: 0.27105\n",
      "* step: 3901 loss: 0.249034\n",
      "* step: 4001 loss: 0.2862\n",
      "* step: 4001 loss: 0.2862\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnooittutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 4101 loss: 1.29418\n",
      "* step: 4201 loss: 0.475831\n",
      "* step: 4301 loss: 0.361456\n",
      "* step: 4401 loss: 0.418863\n",
      "* step: 4501 loss: 0.0760502\n",
      "* step: 4601 loss: 0.284524\n",
      "* step: 4701 loss: 0.173939\n",
      "* step: 4801 loss: 0.231344\n",
      "* step: 4901 loss: 0.192534\n",
      "* step: 5001 loss: 0.387575\n",
      "* step: 5001 loss: 0.387575\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> leennnnnnnnitttttsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 5101 loss: 0.631182\n",
      "* step: 5201 loss: 0.296411\n",
      "* step: 5301 loss: 0.465371\n",
      "* step: 5401 loss: 1.30754\n",
      "* step: 5501 loss: 0.172179\n",
      "* step: 5601 loss: 0.226602\n",
      "* step: 5701 loss: 0.275086\n",
      "* step: 5801 loss: 0.428905\n",
      "* step: 5901 loss: 0.444311\n",
      "* step: 6001 loss: 0.0820987\n",
      "* step: 6001 loss: 0.0820987\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnnnoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 6101 loss: 0.225643\n",
      "* step: 6201 loss: 0.279406\n",
      "* step: 6301 loss: 0.238977\n",
      "* step: 6401 loss: 0.241558\n",
      "* step: 6501 loss: 0.103997\n",
      "* step: 6601 loss: 0.356211\n",
      "* step: 6701 loss: 0.129234\n",
      "* step: 6801 loss: 0.435233\n",
      "* step: 6901 loss: 0.111682\n",
      "* step: 7001 loss: 0.066887\n",
      "* step: 7001 loss: 0.066887\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ttneelnnnnoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 7101 loss: 0.124638\n",
      "* step: 7201 loss: 0.0111444\n",
      "* step: 7301 loss: 0.174306\n",
      "* step: 7401 loss: 0.208955\n",
      "* step: 7501 loss: 0.275467\n",
      "* step: 7601 loss: 0.340186\n",
      "* step: 7701 loss: 0.63195\n",
      "* step: 7801 loss: 0.0801242\n",
      "* step: 7901 loss: 0.0317469\n",
      "* step: 8001 loss: 0.0507553\n",
      "* step: 8001 loss: 0.0507553\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnneelnennoitutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 8101 loss: 0.117383\n",
      "* step: 8201 loss: 0.003143\n",
      "* step: 8301 loss: 0.0760783\n",
      "* step: 8401 loss: 0.0755819\n",
      "* step: 8501 loss: 0.108149\n",
      "* step: 8601 loss: 0.0431165\n",
      "* step: 8701 loss: 0.161774\n",
      "* step: 8801 loss: 0.33127\n",
      "* step: 8901 loss: 0.00246594\n",
      "* step: 9001 loss: 0.583232\n",
      "* step: 9001 loss: 0.583232\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnneellelnnitttitsnocitna (KO)\n",
      "* correct: 38/40 -> 95.0%\n",
      "\n",
      "* step: 9101 loss: 0.0307513\n",
      "* step: 9201 loss: 0.103755\n",
      "* step: 9301 loss: 0.084342\n",
      "* step: 9401 loss: 0.113452\n",
      "* step: 9501 loss: 0.270727\n",
      "* step: 9601 loss: 0.10271\n",
      "* step: 9701 loss: 0.0160936\n",
      "* step: 9801 loss: 0.0617963\n",
      "* step: 9901 loss: 0.19003\n",
      "* step: 10001 loss: 0.921862\n",
      "* step: 10001 loss: 0.921862\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnllnnoittitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 10101 loss: 0.0509589\n",
      "* step: 10201 loss: 0.225025\n",
      "* step: 10301 loss: 0.437091\n",
      "* step: 10401 loss: 0.0733047\n",
      "* step: 10501 loss: 0.0796842\n",
      "* step: 10601 loss: 0.0623753\n",
      "* step: 10701 loss: 0.095511\n",
      "* step: 10801 loss: 0.0178498\n",
      "* step: 10901 loss: 0.015974\n",
      "* step: 11001 loss: 0.0111655\n",
      "* step: 11001 loss: 0.0111655\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> thht (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnellennoitutitsnocitna (KO)\n",
      "* correct: 38/40 -> 95.0%\n",
      "\n",
      "* step: 11101 loss: 0.0259847\n",
      "* step: 11201 loss: 0.0470129\n",
      "* step: 11301 loss: 0.802253\n",
      "* step: 11401 loss: 0.0457759\n",
      "* step: 11501 loss: 0.162938\n",
      "* step: 11601 loss: 0.0859391\n",
      "* step: 11701 loss: 0.150308\n",
      "* step: 11801 loss: 0.587813\n",
      "* step: 11901 loss: 0.142543\n",
      "* step: 12001 loss: 0.114113\n",
      "* step: 12001 loss: 0.114113\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yaal (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> ii (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> thht (KO)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> ll (KO)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ll (KO)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> ii (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gool (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnelllnnnoitutitsnocitna (KO)\n",
      "* correct: 22/40 -> 55.00000000000001%\n",
      "\n",
      "* step: 12101 loss: 0.37469\n",
      "* step: 12201 loss: 0.207097\n",
      "* step: 12301 loss: 0.0442141\n",
      "* step: 12401 loss: 0.031701\n",
      "* step: 12501 loss: 0.0882373\n",
      "* step: 12601 loss: 0.326713\n",
      "* step: 12701 loss: 0.283551\n",
      "* step: 12801 loss: 0.12138\n",
      "* step: 12901 loss: 0.656075\n",
      "* step: 13001 loss: 0.335491\n",
      "* step: 13001 loss: 0.335491\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnlnnnnnottttttitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 13101 loss: 0.330052\n",
      "* step: 13201 loss: 0.229562\n",
      "* step: 13301 loss: 0.0695075\n",
      "* step: 13401 loss: 0.806474\n",
      "* step: 13501 loss: 0.0702468\n",
      "* step: 13601 loss: 0.0485786\n",
      "* step: 13701 loss: 0.01253\n",
      "* step: 13801 loss: 0.0258206\n",
      "* step: 13901 loss: 0.158336\n",
      "* step: 14001 loss: 0.158578\n",
      "* step: 14001 loss: 0.158578\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnntemllnniiutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 14101 loss: 0.0277262\n",
      "* step: 14201 loss: 0.131691\n",
      "* step: 14301 loss: 0.820129\n",
      "* step: 14401 loss: 0.460155\n",
      "* step: 14501 loss: 0.431322\n",
      "* step: 14601 loss: 0.27959\n",
      "* step: 14701 loss: 0.253713\n",
      "* step: 14801 loss: 0.332197\n",
      "* step: 14901 loss: 0.307862\n",
      "* step: 15001 loss: 0.199294\n",
      "* step: 15001 loss: 0.199294\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnennnnnnntttutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 15101 loss: 0.308124\n",
      "* step: 15201 loss: 0.200687\n",
      "* step: 15301 loss: 0.278081\n",
      "* step: 15401 loss: 0.241464\n",
      "* step: 15501 loss: 0.257296\n",
      "* step: 15601 loss: 0.279707\n",
      "* step: 15701 loss: 0.56647\n",
      "* step: 15801 loss: 0.197568\n",
      "* step: 15901 loss: 0.212754\n",
      "* step: 16001 loss: 0.158885\n",
      "* step: 16001 loss: 0.158885\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnennnntttutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 16101 loss: 0.0569713\n",
      "* step: 16201 loss: 0.0362932\n",
      "* step: 16301 loss: 0.302563\n",
      "* step: 16401 loss: 0.0311433\n",
      "* step: 16501 loss: 0.0979653\n",
      "* step: 16601 loss: 0.489605\n",
      "* step: 16701 loss: 0.243303\n",
      "* step: 16801 loss: 0.372788\n",
      "* step: 16901 loss: 0.0955127\n",
      "* step: 17001 loss: 0.68971\n",
      "* step: 17001 loss: 0.68971\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnnenlnnoitsnocitna (KO)\n",
      "* correct: 36/40 -> 90.0%\n",
      "\n",
      "* step: 17101 loss: 0.0678884\n",
      "* step: 17201 loss: 0.0661508\n",
      "* step: 17301 loss: 0.0632216\n",
      "* step: 17401 loss: 0.100619\n",
      "* step: 17501 loss: 0.776421\n",
      "* step: 17601 loss: 0.044243\n",
      "* step: 17701 loss: 0.104002\n",
      "* step: 17801 loss: 0.22472\n",
      "* step: 17901 loss: 0.34834\n",
      "* step: 18001 loss: 0.00153799\n",
      "* step: 18001 loss: 0.00153799\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnneellennoitutitsnocitna (KO)\n",
      "* correct: 38/40 -> 95.0%\n",
      "\n",
      "* step: 18101 loss: 0.168406\n",
      "* step: 18201 loss: 0.0120319\n",
      "* step: 18301 loss: 0.0233645\n",
      "* step: 18401 loss: 0.00543715\n",
      "* step: 18501 loss: 0.00882066\n",
      "* step: 18601 loss: 0.0835313\n",
      "* step: 18701 loss: 0.00472124\n",
      "* step: 18801 loss: 0.00108294\n",
      "* step: 18901 loss: 0.00421503\n",
      "* step: 19001 loss: 0.000786204\n",
      "* step: 19001 loss: 0.000786204\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ttnmellennoitutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 19101 loss: 0.0194964\n",
      "* step: 19201 loss: 0.000611049\n",
      "* step: 19301 loss: 0.000521832\n",
      "* step: 19401 loss: 0.00834921\n",
      "* step: 19501 loss: 0.000628707\n",
      "* step: 19601 loss: 0.0102531\n",
      "* step: 19701 loss: 0.00075871\n",
      "* step: 19801 loss: 0.0028611\n",
      "* step: 19901 loss: 0.000673249\n",
      "* step: 20001 loss: 0.00811554\n",
      "* step: 20001 loss: 0.00811554\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnmellennoitutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 20101 loss: 0.0315707\n",
      "* step: 20201 loss: 0.00907248\n",
      "* step: 20301 loss: 0.0243528\n",
      "* step: 20401 loss: 0.0353656\n",
      "* step: 20501 loss: 0.00528631\n",
      "* step: 20601 loss: 0.000486432\n",
      "* step: 20701 loss: 0.00058689\n",
      "* step: 20801 loss: 0.000553071\n",
      "* step: 20901 loss: 0.00387147\n",
      "* step: 21001 loss: 0.00570368\n",
      "* step: 21001 loss: 0.00570368\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ttnnellennoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 21101 loss: 0.00206219\n",
      "* step: 21201 loss: 0.000298567\n",
      "* step: 21301 loss: 0.00183277\n",
      "* step: 21401 loss: 0.0174071\n",
      "* step: 21501 loss: 0.0261878\n",
      "* step: 21601 loss: 0.0100293\n",
      "* step: 21701 loss: 0.278922\n",
      "* step: 21801 loss: 0.00813735\n",
      "* step: 21901 loss: 0.00172691\n",
      "* step: 22001 loss: 0.650064\n",
      "* step: 22001 loss: 0.650064\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ttnnnnntitnninnitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 22101 loss: 0.572044\n",
      "* step: 22201 loss: 0.070672\n",
      "* step: 22301 loss: 0.0929677\n",
      "* step: 22401 loss: 0.00134449\n",
      "* step: 22501 loss: 0.00778491\n",
      "* step: 22601 loss: 0.00988542\n",
      "* step: 22701 loss: 0.0682192\n",
      "* step: 22801 loss: 0.00208524\n",
      "* step: 22901 loss: 0.224953\n",
      "* step: 23001 loss: 0.026383\n",
      "* step: 23001 loss: 0.026383\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnellennoitutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 23101 loss: 0.0773626\n",
      "* step: 23201 loss: 0.0364474\n",
      "* step: 23301 loss: 0.00140166\n",
      "* step: 23401 loss: 0.0246236\n",
      "* step: 23501 loss: 0.00134779\n",
      "* step: 23601 loss: 0.0131511\n",
      "* step: 23701 loss: 0.0187241\n",
      "* step: 23801 loss: 0.0116516\n",
      "* step: 23901 loss: 0.059857\n",
      "* step: 24001 loss: 0.179321\n",
      "* step: 24001 loss: 0.179321\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnelellnnnoitutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 24101 loss: 0.00136936\n",
      "* step: 24201 loss: 0.000771283\n",
      "* step: 24301 loss: 0.00103034\n",
      "* step: 24401 loss: 0.0008523\n",
      "* step: 24501 loss: 0.221034\n",
      "* step: 24601 loss: 0.00576699\n",
      "* step: 24701 loss: 0.000677115\n",
      "* step: 24801 loss: 0.00601807\n",
      "* step: 24901 loss: 0.000669698\n",
      "* step: 25001 loss: 1.34079\n",
      "* step: 25001 loss: 1.34079\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhilgne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnlllnnnoittitsnocitna (KO)\n",
      "* correct: 36/40 -> 90.0%\n",
      "\n",
      "* step: 25101 loss: 0.118106\n",
      "* step: 25201 loss: 0.00987155\n",
      "* step: 25301 loss: 0.0115868\n",
      "* step: 25401 loss: 0.00195686\n",
      "* step: 25501 loss: 0.000765494\n",
      "* step: 25601 loss: 0.0124353\n",
      "* step: 25701 loss: 0.000946162\n",
      "* step: 25801 loss: 0.00124977\n",
      "* step: 25901 loss: 0.00108592\n",
      "* step: 26001 loss: 0.000939455\n",
      "* step: 26001 loss: 0.000939455\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnemellennoitutitsnocitna (OK)\n",
      "* correct: 40/40 -> 100.0%\n",
      "\n",
      "* step: 26101 loss: 0.0187867\n",
      "* step: 26201 loss: 0.000505525\n",
      "* step: 26301 loss: 0.000462939\n",
      "* step: 26401 loss: 0.00540839\n",
      "* step: 26501 loss: 0.000840944\n",
      "* step: 26601 loss: 0.00056493\n",
      "* step: 26701 loss: 0.00122591\n",
      "* step: 26801 loss: 0.000592862\n",
      "* step: 26901 loss: 0.000378572\n",
      "* step: 27001 loss: 0.000475079\n",
      "* step: 27001 loss: 0.000475079\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhilgne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnemellennoitutitsnocitna (OK)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 27101 loss: 0.000500963\n",
      "* step: 27201 loss: 0.000337351\n",
      "* step: 27301 loss: 0.000842192\n",
      "* step: 27401 loss: 0.000285498\n",
      "* step: 27501 loss: 0.000333684\n",
      "* step: 27601 loss: 0.000470919\n",
      "* step: 27701 loss: 0.000388047\n",
      "* step: 27801 loss: 0.0640065\n",
      "* step: 27901 loss: 0.0177828\n",
      "* step: 28001 loss: 0.000570862\n",
      "* step: 28001 loss: 0.000570862\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnemellennoitutitsnocitna (OK)\n",
      "* correct: 40/40 -> 100.0%\n",
      "\n",
      "* step: 28101 loss: 0.000331725\n",
      "* step: 28201 loss: 0.000499372\n",
      "* step: 28301 loss: 0.000540483\n",
      "* step: 28401 loss: 0.000710103\n",
      "* step: 28501 loss: 0.000236875\n",
      "* step: 28601 loss: 0.17214\n",
      "* step: 28701 loss: 0.000458054\n",
      "* step: 28801 loss: 0.000357569\n",
      "* step: 28901 loss: 0.11595\n",
      "* step: 29001 loss: 0.014438\n",
      "* step: 29001 loss: 0.014438\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhilgne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnneellennoitutitsnocitna (KO)\n",
      "* correct: 38/40 -> 95.0%\n",
      "\n",
      "* step: 29101 loss: 0.00073149\n",
      "* step: 29201 loss: 0.00051337\n",
      "* step: 29301 loss: 0.00794247\n",
      "* step: 29401 loss: 0.00167339\n",
      "* step: 29501 loss: 0.000447664\n",
      "* step: 29601 loss: 0.00160337\n",
      "* step: 29701 loss: 0.00208222\n",
      "* step: 29801 loss: 0.00132329\n",
      "* step: 29901 loss: 0.000579602\n",
      "*** evaluation! loss: 0.00225531\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhilgne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnemellennoitutitsnocitna (OK)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "CPU times: user 1h 25min 35s, sys: 23min 12s, total: 1h 48min 48s\n",
      "Wall time: 33min 42s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "%time reverse_text(30001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
